# WARNING - Generated by {fusen} from dev/flat_ai_reporting.Rmd: do not edit by hand # nolint: line_length_linter.

#' Generate Plot Description with AI (Phase 3)
#'
#' This function implements Phase 3 of the AI reporting architecture.
#' It serves as the "Semantic Generator," combining structure and statistics
#' to prompt an LLM for a structured description.
#'
#' @param structure Metadata list returned by `extract_structure`.
#' @param stats Statistical profile returned by `profile_data`.
#' @param provider The LLM provider (e.g., "openai", "azure", "anthropic").
#' @param model The specific model to use.
#' @param max_tokens Maximum token limit for the response.
#' @return A list containing:
#'   \item{short_desc}{A concise, WCAG-compliant alt text string.}
#'   \item{long_desc}{A detailed analytical description of the visualization.}
#' @importFrom ellmer chat_openai chat_google_gemini chat_anthropic
#' @importFrom ellmer chat_ollama chat_azure_openai
#' @importFrom utils capture.output
#' @importFrom jsonlite fromJSON
#' @export
generate_description <- function(
  structure,
  stats,
  provider = NULL,
  model = NULL,
  max_tokens = 500
) {
  # Construct Context
  context_str <- paste0(
    "PLOT METADATA:\n",
    "Title: ",
    structure$labels$title,
    "\n",
    "Subtitle: ",
    structure$labels$subtitle,
    "\n",
    "Geoms: ",
    paste(unique(structure$geoms), collapse = ", "),
    "\n",
    "X Label: ",
    structure$labels$x,
    "\n",
    "Y Label: ",
    structure$labels$y,
    "\n\n",

    "STATISTICAL PROFILE:\n",
    "STATISTICAL PROFILE:\n",
    paste(capture.output(print(stats$distributions)), collapse = "\n"),
    "\n",
    "Correlations: ",
    paste(
      names(stats$correlations),
      unlist(stats$correlations),
      sep = ": ",
      collapse = ", "
    ),
    "\n"
  )

  system_prompt <- paste0(
    "You are a Senior Humanitarian Data Storyteller for UNHCR. ",
    "Your goal is to influence decision-makers by turning data into a 'Cause and Effect' narrative.\n\n",

    "Your task is to generate two outputs as a strict JSON object:\n\n",

    "1. 'short_desc': A concise, WCAG-compliant alt text ",
    "(e.g., '* [Chart Type] of [Variables] showing [Trend]*').\n",

    "2. 'long_desc': A powerful narrative (3-5 sentences) that follows this arc:\n",
    "   - **The Setup (Context):** Briefly set the scene using the data (e.g., 'As regional instability deepens...').\n",
    "   - **The Conflict (Insight):** Identify the tension or key shift. Connect the 'Cause' (the numbers) to the 'Effect' (the human impact). ",
    "     *Example: 'The sudden surge in arrivals [Cause] has overwhelmed reception capacity [Effect].'*\n",
    "   - **The Resolution (Implication):** Conclude with the 'So What?'â€”why does this matter for policy or action?\n\n",

    "   **Constraints:**\n",
    "   - Use strong, active verbs (e.g., 'accelerated', 'stalled', 'concentrated').\n",
    "   - DO NOT describe the chart visuals (e.g., 'The x-axis shows', 'The bar chart displays').\n",
    "   - Weave statistics naturally into the story; do not list them."
  )

  prompt <- paste0(
    "Context:\n",
    context_str,
    "\n\n",
    "Task: Interpret this data to inspire action. \n",
    "Identify the 'Cause and Effect' relationship in the numbers. ",
    "Tell the story of what is happening to the people represented in this dataset."
  )

  # Logic to select provider
  # (duplicated from generate_plot_story for now to adhere to modularity)
  if (is.null(provider)) {
    if (!is.na(Sys.getenv("AZURE_OPENAI_ENDPOINT", unset = NA_character_))) {
      provider <- "azure"
    } else if (!is.na(Sys.getenv("OPENAI_API_KEY", unset = NA_character_))) {
      provider <- "openai"
    } else if (!is.na(Sys.getenv("GEMINI_API_KEY", unset = NA_character_))) {
      provider <- "gemini"
    } else if (!is.na(Sys.getenv("ANTHROPIC_API_KEY", unset = NA_character_))) {
      provider <- "anthropic"
    } else {
      stop("No supported API key found.")
    }
  }

  provider <- tolower(provider)
  if (is.null(model)) {
    model <- switch(
      provider,
      openai = "gpt-4o-mini",
      gemini = "gemini-2.0-flash",
      anthropic = "claude-3-5-sonnet-latest",
      ollama = "deepseek-r1",
      azure = "gpt-4",
      stop("Invalid provider")
    )
  }

  chat <- switch(
    provider,
    openai = ellmer::chat_openai(
      model = model,
      system_prompt = system_prompt,
      type = "json_object"
    ),
    azure = {
      azure_key <- Sys.getenv("AZURE_OPENAI_API_KEY")
      azure_endpoint <- Sys.getenv("AZURE_OPENAI_ENDPOINT")
      azure_version <- Sys.getenv("AZURE_OPENAI_API_VERSION")
      ellmer::chat_azure_openai(
        system_prompt = system_prompt,
        model = model,
        api_version = azure_version,
        endpoint = azure_endpoint,
        api_key = azure_key,
        type = "json_object"
      )
    },
    gemini = ellmer::chat_google_gemini(
      model = model,
      system_prompt = system_prompt,
      api_key = Sys.getenv("GEMINI_API_KEY")
    ),
    anthropic = ellmer::chat_anthropic(
      model = model,
      system_prompt = system_prompt
    ),
    ollama = ellmer::chat_ollama(model = model, system_prompt = system_prompt),
    stop("Invalid provider")
  )

  response <- tryCatch(
    {
      chat$chat(prompt)
    },
    error = function(e) {
      paste("Error invoking AI provider:", e$message)
    }
  )

  # Parse JSON
  # Clean potential markdown code blocks if the model insists on adding them
  cleaned_json <- gsub("^```json\\s*|\\s*```$", "", response)

  tryCatch(
    {
      jsonlite::fromJSON(cleaned_json)
    },
    error = function(e) {
      list(short_desc = "Error parsing JSON", long_desc = response)
    }
  )
}
