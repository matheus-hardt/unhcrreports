# WARNING - Generated by {fusen} from dev/flat_ai_reporting.Rmd: do not edit by hand # nolint: line_length_linter.

#' Generate Humanitarian Data Story from ggplot
#'
#' This function takes a ggplot2 object and generates a storytelling narrative
#' focused on humanitarian insights. It uses the \{ellmer\} package to call
#' a large or small language model from a supported provider.
#' 
#' If you do not have API keys or need to work offline, simply get ollama and 
#' look at top reasoning models - https://ollama.com/search?c=thinking 
#'
#' Setup:
#' 1. Install \{ellmer\}: `install.packages("ellmer")`
#' 2. Set your API key in your environment. For Azure OpenAI, use the standard
#'    OpenAI key variable:
#'    `Sys.setenv(OPENAI_API_KEY = "<YOUR_AZURE_OPENAI_KEY>")`
#' 3. When using Azure, set `provider = "azure"` and provide the env variables
#'   `Sys.setenv(AZURE_OPENAI_ENDPOINT = "<YOUR_AZURE_ENDPOINT>")`
#'   `Sys.setenv(AZURE_OPENAI_API_VERSION = "<YOUR_AZURE_OPENAI_API_VERSION>")`
#'   The best place to set this is in .Renviron, which you can easily edit by
#'    calling `usethis::edit_r_environ()`
#'
#' @param plot A `ggplot` object from ggplot2.
#' @param max_tokens Maximum number of tokens (approximate) 
#' for the narrative (default = 30).
#' @param provider Optional character string specifying the provider. Options 
#' include:
#'   `"openai"`, `"gemini"`, `"anthropic"`, `"ollama"`, `"azure"`. If `NULL`, 
#'   auto-detect from environment keys.
#' @param model Optional character string specifying the model name. For Azure,
#'    this is typically the deployment name. If `NULL`, a default model for the
#'    chosen provider will be used.
#' @param clean_response Logical. Whether to clean the response by removing
#'   thinking tags and other artifacts (default = TRUE).
#'
#' @return A character string containing a storytelling narrative focused on
#'  humanitarian data.
#'
#' @importFrom ggplot2 ggplot_build
#' @importFrom dplyr mutate_if
#' @importFrom utils capture.output head
#' @importFrom ellmer chat_openai chat_google_gemini chat_anthropic chat_ollama chat_azure_openai
#' @export
#' @examples
#'
#' library(ggplot2)
#' p <- ggplot(mtcars, aes(x = wt, y = mpg)) +
#'    geom_point() +
#'     unhcrthemes::theme_unhcr(grid = "Y", axis = "X", axis_title = FALSE) +
#'    labs(title = "Vehicle Efficiency",
#'         subtitle = "Fuel consumption vs weight",
#'         caption = "Source: mtcars dataset")
#'
#' generate_plot_story(p, provider = "ollama", model = "deepseek-r1")
#'
#' story <- generate_plot_story(p, provider = "azure", model = "gpt-4.1-mini", max_tokens = 300)
#' # To use as subtitle:
#' p + ggplot2::labs(subtitle = story)
generate_plot_story <- function(plot, 
                                max_tokens = 30,
                                provider = NULL,
                                model = NULL,
                                clean_response = TRUE) {
  
  # Extract plot data (first layer) and truncate
  plot_data <- ggplot2::ggplot_build(plot)$data[[1]] |>
    dplyr::mutate_if(is.numeric, round, 2) |>
    head(30)
  plot_data_text <- capture.output(print(plot_data))
  
  # Extract title, subtitle, caption
  labels <- plot$labels
  title    <- if (!is.null(labels$title))    labels$title    else ""
  subtitle <- if (!is.null(labels$subtitle)) labels$subtitle else ""
  caption  <- if (!is.null(labels$caption))  labels$caption  else ""
  
  # Extract mapping aesthetics
  mapping_info <- if (!is.null(plot$mapping)) {
    mapping_text <- capture.output(print(plot$mapping))
    paste("Global mapping:", paste(mapping_text, collapse = " "))
  } else {
    "No global mapping defined"
  }

  # Extract layer-specific mappings
  layer_mappings <- sapply(plot$layers, function(layer) {
    if (!is.null(layer$mapping)) {
      paste("Layer mapping:", capture.output(print(layer$mapping)))
    } else {
      "No layer-specific mapping"
    }
  })
  layer_mappings_text <- paste(unique(layer_mappings), collapse = "; ")
  
  # Detect scales and transformations
  scale_info <- if (!is.null(plot$scales$scales) && length(plot$scales$scales) > 0) {
    scale_types <- sapply(plot$scales$scales, function(scale) {
      paste(class(scale)[1], "scale (", scale$aesthetics[1], ")")
    })
    paste("Scale transformations:", paste(unique(scale_types), collapse = ", "))
  } else {
    "No custom scale transformations detected"
  }
  
  # Detect geoms used
  geoms <- unique(sapply(plot$layers, function(layer) class(layer$geom)[1]))
  geoms_text <- paste(geoms, collapse = ", ")
  
  system_prompt <- paste0(
    "You are a humanitarian data analysis expert, working for UNHCR and specialised in Statistics on Forced Displacement. Your role is to: \n",
    "1. Extract, from a given dataset and its visualization parameters, key insights with quotable and specific data points \n",
    "2. Create compelling, accessible narratives tailored for a humanitarian proposal writer audience \n",
    "3. Highlight patterns, trends, and implications to consider when looking for funding opportunities \n",
    "4. Adapt interpretation depth based on available data and context \n",
    "5. Use clear, plain language with appropriate practical recommendations \n\n",
    
    "CRITICAL INSTRUCTIONS:\n",
    "- DO NOT include any thinking tags like <think> or </think> in your response\n",
    "- DO NOT use markdown formatting, asterisks, or special characters\n",
    "- Provide ONLY the final narrative text\n",
    "- Your response should be clean plain text ready for display\n\n"
    
  )
  
  # Build enhanced prompt
  prompt <- paste0(
    "VISUALIZATION CONTEXT:\n",
    "Consider what each mapped variable represents and any data transformations applied.\n\n",
    "Title: ", title, "\n",
    "Subtitle: ", subtitle, "\n", 
    "Caption: ", caption, "\n",
    "Chart type(s): ", geoms_text, "\n",
    "Data mappings: ", mapping_info, "\n",
    "Layer mappings: ", layer_mappings_text, "\n", 
    "Scales: ", scale_info, "\n\n",
    
    "Use the following DATA:\n", paste(plot_data_text, collapse = "\n"), "\n\n",
    
    "TASK: Create a fund raising analysis using the provided data and visualization parameters.\n",
    
    "ADAPTIVE CONSTRAINTS:\n",
    "- Important! Maximum length for the generated output: ", max_tokens, " tokens\n",
    "- Prioritize clarity over completeness\n",
    "- Focus on 2-3 key insights if space is limited\n",
    "- Use concise but impactful language\n",
    "- IMPORTANT: Provide only clean text output without any thinking tags or markdown\n\n"
    
  )
  
  # Auto-detect provider if not specified
  if (is.null(provider)) {
    # Check for Azure-specific environment variables
    if (!is.na(Sys.getenv("AZURE_OPENAI_ENDPOINT", unset = NA_character_)) &&
        !is.na(Sys.getenv("AZURE_OPENAI_API_KEY", unset = NA_character_))) {
      provider <- "azure"
    } else if (!is.na(Sys.getenv("OPENAI_API_KEY", unset = NA_character_))) {
      provider <- "openai"
    } else if (!is.na(Sys.getenv("GEMINI_API_KEY", unset = NA_character_))) {
      provider <- "gemini"
    } else if (!is.na(Sys.getenv("ANTHROPIC_API_KEY", unset = NA_character_))) {
      provider <- "anthropic"
    } else {
      stop("No supported API key found. Set AZURE_OPENAI_ENDPOINT/KEY, OPENAI_API_KEY, GEMINI_API_KEY,
           ANTHROPIC_API_KEY, or install and set it to a local OLLAMA")
    }
  }
  
  provider <- tolower(provider)
  
  # Set default models if not provided
  if (is.null(model)) {
    model <- switch(
      provider,
      openai = "gpt-4o-mini",
      gemini = "gemini-2.5-flash",
      anthropic = "claude-3-5-sonnet-20241022",
      ollama = "deepseek-r1",
      azure = "gpt-4", # Placeholder: Must be a valid deployment name
      stop("Invalid provider specified. Choose from 
           'openai', 'gemini', 'anthropic', 'ollama', 'azure'.")
    )
  }
  
  # Initialize chat object
  chat <- switch(
    provider,
    openai = ellmer::chat_openai(model = model, system_prompt = system_prompt),
    
    # Azure OpenAI using the dedicated function and explicit environment variable checks
    azure = {
      # Fetch required environment variables
      azure_key <- Sys.getenv("AZURE_OPENAI_API_KEY")
      azure_endpoint <- Sys.getenv("AZURE_OPENAI_ENDPOINT")
      azure_version <- Sys.getenv("AZURE_OPENAI_API_VERSION")
      
      # VALIDATION: Check if any required variable is unset (returns "")
      if (azure_key == "" || azure_endpoint == "" || azure_version == "") {
        stop("For 'azure' provider, AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, and AZURE_OPENAI_VERSION environment variables must all be set correctly in the R session.")
      }
      
      # Initialize chat object, passing the validated environment variables
      ellmer::chat_azure_openai(
        system_prompt = system_prompt,
        model = model,
        api_version = azure_version,
        endpoint = azure_endpoint,
        api_key = azure_key
      )
    },
    
    gemini = ellmer::chat_google_gemini(
      model = model,
      system_prompt = system_prompt,
      base_url = "https://generativelanguage.googleapis.com/v1beta/",
      api_key = Sys.getenv("GEMINI_API_KEY")),
    
    anthropic = ellmer::chat_anthropic(
      model = model, 
      system_prompt = system_prompt),
    
    ollama = ellmer::chat_ollama(
      model = model,
      system_prompt = system_prompt),
    
    stop(
      "Invalid provider specified. Choose from
         'openai', 'gemini', 'anthropic', 'ollama', 'azure'."
    )
  )
  
  # Send prompt and get response
  response <- chat$chat(prompt)
  
  # Clean response if requested
  if (clean_response) {
    response <- clean_llm_response(response)
  }
  
  return(response)
}

