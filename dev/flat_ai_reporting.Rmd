---
title: "AI Reporting and Rendering Functions"
output: html_document
editor_options:
  chunk_output_type: console
---


```{r development, include=FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  message = FALSE,
  warning = FALSE,
  # fig.retina = 2,
  fig.width = 8,
  fig.asp = 0.718,
  # fig.align = "center",
  # dev = "ragg_png",
  out.width = "90%"
)
unlink(".RData")
library(testthat)
library(ggplot2)
library(tidyverse)
library(scales)
library(unhcrthemes)
# library(unhcrdatapackage)
# library(extrafont)
# font_import()
# loadfonts()
# font_install('fontcm')
# usethis::use_package("unhcrtemplate", type = "Suggests")
```

```{r development-load}
# Load already included functions if relevant
pkgload::load_all(export_all = FALSE)
```


# Using Language Models to build data stories

## clean_llm_response

```{r function-clean_llm_response}
#' Clean LLM Response
#'
#' Remove thinking tags, markdown, and other artifacts from LLM responses
#' Useful for Local Open Source Reasoning Small Language Model
#'
#' @param response Character string from LLM response
#' @param keep_punctuation boolean
#' @return Cleaned character string
#' @export
clean_llm_response <- function(response, keep_punctuation = TRUE) {
  if (!is.character(response)) {
    return(response)
  }

  # Split into lines
  lines <- strsplit(response, "\n")[[1]]

  # Find thinking tag boundaries
  think_start <- which(grepl("<think>", lines, ignore.case = TRUE))
  think_end <- which(grepl("</think>", lines, ignore.case = TRUE))

  # Remove ALL lines between thinking tags (inclusive)
  if (length(think_start) > 0 && length(think_end) > 0) {
    remove_indices <- integer(0)
    for (i in seq_along(think_start)) {
      if (think_start[i] <= think_end[i]) {
        remove_indices <- c(remove_indices, think_start[i]:think_end[i])
      }
    }
    if (length(remove_indices) > 0) {
      lines <- lines[-remove_indices]
    }
  }

  # Combine back
  response <- paste(lines, collapse = " ")

  # Remove all non-ASCII characters
  response <- gsub("[^\x20-\x7E]", "", response)

  # Remove common LLM artifacts and introductory phrases
  response <- gsub("^(Sure|Certainly|Okay|Here|First).*?(:|\\.)\\s*", "", response, ignore.case = TRUE)
  response <- gsub("^(As an AI|I am an AI|I'm a).*?\\.\\s*", "", response, ignore.case = TRUE)
  response <- gsub("^(My role includes|This includes).*?\\.\\s*", "", response, ignore.case = TRUE)
  response <- gsub("^(Aligning with constraints).*?\\.\\s*", "", response, ignore.case = TRUE)

  # Remove markdown formatting
  response <- gsub("\\*\\*|\\*|__|_", "", response)
  response <- gsub("\\[.*?\\]\\(.*?\\)", "", response)
  response <- gsub("#+\\s*", "", response)
  response <- gsub("`{1,3}", "", response)

  # Remove excessive whitespace
  response <- gsub("\\s+", " ", response)
  response <- trimws(response)

  # If empty after cleaning, provide fallback
  if (nchar(response) == 0) {
    return("Unable to generate story from this visualization.")
  }

  return(response)
}
```
  
```{r example-clean_llm_response}
response <- "<think>
First, I'm a humanitarian data visualization expert. My role includes extracting insights
from visualizations, creating accessible narratives, highlighting patterns relevant to aid
efforts, using clear language with emotional resonance.
Aligning with constraints: Use plain language, be concise and impactful. Don't rehash
every detail; build narrative depth around 2 key insights maximum in under 30 tokens.
</think>
This visualization tracks a relationship potentially critical for humanitarian logistics:
higher fuel consumption versus increased weight. 车辆设计"
clean_llm_response(response)
```
  
```{r tests-clean_llm_response}
test_that("clean_llm_response works", {
  expect_true(inherits(clean_llm_response, "function"))
})
```
  


# Three-Tier Architecture for AI Reporting

The AI reporting toolkit uses a modular architecture to analyze ggplot2 objects and generate context-aware narratives. This approach ensures that the description is grounded in the actual visual structure and data of the plot.

1.  **Phase 1: Structural Extractor (`extract_structure`)**
    Interrogates the rendered plot object (using `ggplot_build`) to retrieve "trained" metadata, such as exact axis ranges, visible labels, and legend mappings. This is the "ground truth" of what the user sees.

2.  **Phase 2: Statistical Profiler (`profile_data`)**
    Summarizes the underlying data distributions and, where appropriate (e.g., scatterplots), calculates statistical relationships like correlations. This provides the "data context" that might not be immediately obvious visually.

3.  **Phase 3: Semantic Generator (`generate_description`)**
    Combines the structural metadata and statistical profile into a structured prompt for the LLM. It returns a JSON object containing a WCAG-compliant short description (alt text) and a detailed long description.

## extract_structure

```{r function-extract_structure}
#' Extract Plot Structure and Metadata (Phase 1)
#'
#' This function implements Phase 1 of the AI reporting architecture.
#' It extracts visual metadata from a ggplot object by forcing a render build.
#' Unlike simple label extraction, this captures the "trained" ranges and
#' legend mappings that are actually displayed to the user.
#'
#' @param p A `ggplot` object.
#' @return A list containing:
#'   \item{labels}{Title, subtitle, caption, and axis labels.}
#'   \item{ranges}{Exact x and y ranges for each panel (trained).}
#'   \item{guides}{Mapping of visuals (color/shape) to data values.}
#'   \item{geoms}{List of geometric layers used.}
#' @importFrom ggplot2 ggplot_build get_guide_data
#' @importFrom purrr map map_chr
#' @export
extract_structure <- function(p) {
  if (is.null(p)) return(NULL)
  
  # Force layout build to get trained ranges
  built <- ggplot2::ggplot_build(p)
  
  # Extract trained axis ranges (handling facets)
  layout_ranges <- built$layout$panel_params |>
    purrr::map(function(panel) {
      list(
        x_range = panel$x.range,
        y_range = panel$y.range
      )
    })
  
  # Decode legends using get_guide_data (available in ggplot2 >= 3.5.0)
  guides_map <- tryCatch({
    ggplot2::get_guide_data(p)
  }, error = function(e) {
    NULL
  })
  
  # Basic geoms
  geoms <- purrr::map_chr(p$layers, ~ class(.x$geom)[1])
  
  list(
    labels = p$labels,
    ranges = layout_ranges,
    guides = guides_map,
    geoms = geoms,
    scales = p$scales$scales
  )
}
```

## profile_data

```{r function-profile_data}
#' Profile Plot Data (Phase 2)
#'
#' This function implements Phase 2 of the AI reporting architecture.
#' It generates a statistical profile of the data used in the plot to provide
#' context for the AI.
#'
#' @details
#' It performs two main tasks:
#' 1. **Distribution Analysis**: Uses `skimr::skim()` to summarize variables mapped in the plot.
#' 2. **Correlation Check**: For scatterplots (`geom_point`), it calculates Pearson correlations
#'    to help the AI identify relationships.
#'
#' @param p A `ggplot` object.
#' @return A list containing:
#'   \item{distributions}{A `skim_df` object summarizing mapped variables.}
#'   \item{correlations}{A list of correlation coefficients (if applicable).}
#' @importFrom skimr skim
#' @importFrom stats cor
#' @export
profile_data <- function(p) {
  if (is.null(p) || is.null(p$data)) return(NULL)
  
  df <- p$data
  
  # Distribution analysis using skimr
  # We focus on variables actually mapped in aes
  mapped_vars <- unique(unlist(purrr::map(p$mapping, all.vars)))
  mapped_vars <- mapped_vars[mapped_vars %in% names(df)]
  
  if (length(mapped_vars) > 0) {
    df_mapped <- df[mapped_vars]
  } else {
    df_mapped <- df
  }
  
  # Capture skimr output
  dist_summary <- skimr::skim(df_mapped)
  
  # Correlation check for Scatterplots
  correlations <- list()
  has_points <- any(sapply(p$layers, function(l) inherits(l$geom, "GeomPoint")))
  
  if (has_points && "x" %in% names(p$mapping) && "y" %in% names(p$mapping)) {
    x_vars <- all.vars(p$mapping$x)
    y_vars <- all.vars(p$mapping$y)
    
    if (length(x_vars) == 1 && length(y_vars) == 1) {
      x_var <- x_vars
      y_var <- y_vars
      
      # Check if they exist in data and are numeric
      if (x_var %in% names(df) && y_var %in% names(df) && 
          is.numeric(df[[x_var]]) && is.numeric(df[[y_var]])) {
        cor_val <- stats::cor(df[[x_var]], df[[y_var]], use = "complete.obs")
        correlations[[paste(x_var, "vs", y_var)]] = cor_val
      }
    }
  }
  
  list(
    distributions = dist_summary,
    correlations = correlations
  )
}
```

## generate_description

```{r function-generate_description}
#' Generate Plot Description with AI (Phase 3)
#'
#' This function implements Phase 3 of the AI reporting architecture.
#' It serves as the "Semantic Generator," combining structure and statistics
#' to prompt an LLM for a structured description.
#'
#' @param structure Metadata list returned by `extract_structure`.
#' @param stats Statistical profile returned by `profile_data`.
#' @param provider The LLM provider (e.g., "openai", "azure", "anthropic").
#' @param model The specific model to use.
#' @param max_tokens Maximum token limit for the response.
#' @return A list containing:
#'   \item{short_desc}{A concise, WCAG-compliant alt text string.}
#'   \item{long_desc}{A detailed analytical description of the visualization.}
#' @importFrom ellmer chat_openai chat_google_gemini chat_anthropic chat_ollama chat_azure_openai
#' @importFrom utils capture.output
#' @importFrom jsonlite fromJSON
#' @export
generate_description <- function(structure, stats, provider = NULL, model = NULL, max_tokens = 500) {
  
  # Construct Context
  context_str <- paste0(
    "PLOT METADATA:\n",
    "Title: ", structure$labels$title, "\n",
    "Subtitle: ", structure$labels$subtitle, "\n",
    "Geoms: ", paste(unique(structure$geoms), collapse = ", "), "\n",
    "X Label: ", structure$labels$x, "\n",
    "Y Label: ", structure$labels$y, "\n\n",
    
    "STATISTICAL PROFILE:\n",
    capture.output(print(stats$distributions)) |> paste(collapse = "\n"), "\n",
    "Correlations: ", paste(names(stats$correlations), unlist(stats$correlations), sep=": ", collapse = ", "), "\n"
  )
  
  system_prompt <- paste0(
    "You are an expert accessibility consultant and data analyst for UNHCR. ",
    "Your task is to generate two outputs for a given data visualization:\n",
    "1. 'short_desc': A WCAG-compliant alt text following the formula '* [Chart Type] of [Variables], where [Trend/Key Insight]*'.\n",
    "2. 'long_desc': A detailed statistical analysis and context description.\n",
    "Return the result as a strict JSON object with keys 'short_desc' and 'long_desc'."
  )
  
  prompt <- paste0(
    "Context:\n", context_str, "\n\n",
    "Task: Generate the JSON object containing short_desc and long_desc based on the provided context."
  )
  
  # Logic to select provider (duplicated from generate_plot_story for now to adhere to modularity)
  if (is.null(provider)) {
     if (!is.na(Sys.getenv("AZURE_OPENAI_ENDPOINT", unset = NA_character_))) provider <- "azure"
     else if (!is.na(Sys.getenv("OPENAI_API_KEY", unset = NA_character_))) provider <- "openai"
     else if (!is.na(Sys.getenv("GEMINI_API_KEY", unset = NA_character_))) provider <- "gemini"
     else if (!is.na(Sys.getenv("ANTHROPIC_API_KEY", unset = NA_character_))) provider <- "anthropic"
     else stop("No supported API key found.")
  }
  
  provider <- tolower(provider)
  if (is.null(model)) {
    model <- switch(provider,
      openai = "gpt-4o-mini",
      gemini = "gemini-2.0-flash",
      anthropic = "claude-3-5-sonnet-latest",
      ollama = "deepseek-r1",
      azure = "gpt-4",
      stop("Invalid provider")
    )
  }
  
  chat <- switch(provider,
    openai = ellmer::chat_openai(model = model, system_prompt = system_prompt, type = "json_object"),
    azure = {
       azure_key <- Sys.getenv("AZURE_OPENAI_API_KEY")
       azure_endpoint <- Sys.getenv("AZURE_OPENAI_ENDPOINT")
       azure_version <- Sys.getenv("AZURE_OPENAI_API_VERSION")
       ellmer::chat_azure_openai(system_prompt = system_prompt, model = model, 
                                 api_version = azure_version, endpoint = azure_endpoint, api_key = azure_key, type = "json_object")
    },
    gemini = ellmer::chat_google_gemini(model = model, system_prompt = system_prompt, api_key = Sys.getenv("GEMINI_API_KEY")),
    anthropic = ellmer::chat_anthropic(model = model, system_prompt = system_prompt),
    ollama = ellmer::chat_ollama(model = model, system_prompt = system_prompt),
    stop("Invalid provider")
  )
  
  response <- chat$chat(prompt)
  
  # Parse JSON
  # Clean potential markdown code blocks if the model insists on adding them
  cleaned_json <- gsub("^```json\\s*|\\s*```$", "", response)
  
  tryCatch({
    jsonlite::fromJSON(cleaned_json)
  }, error = function(e) {
    list(short_desc = "Error parsing JSON", long_desc = response)
  })
}
```

## generate_plot_story


```{r function-generate_plot_story}

#' Generate Humanitarian Data Story from ggplot
#'
#' This function takes a ggplot2 object and generates a storytelling narrative.
#' It now uses a modular architecture (extract_structure, profile_data, generate_description).
#'
#' @param plot A `ggplot` object.
#' @param max_tokens Max tokens for description.
#' @param provider LLM provider.
#' @param model LLM model.
#' @param clean_response Deprecated. Response is now structured JSON.
#'
#' @return A list containing `$short_desc` and `$long_desc`.
#' @export
generate_plot_story <- function(plot,
                                max_tokens = 300,
                                provider = NULL,
                                model = NULL,
                                clean_response = TRUE) {
                                  
  if (is.null(plot) || !inherits(plot, "ggplot")) {
    return(list(short_desc = "Invalid input", long_desc = "Plot is NULL or not a ggplot object."))
  }
  
  # Phase 1: Structure
  structure <- extract_structure(plot)
  
  # Phase 2: Profile
  stats <- profile_data(plot)
  
  # Phase 3: Semantic Generation
  description <- generate_description(structure, stats, provider, model, max_tokens)
  
  return(description)
}
```
```
  
```{r example-generate_plot_story, eval=FALSE}
library(ggplot2)
p <- ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  unhcrthemes::theme_unhcr(grid = "Y", axis = "X", axis_title = FALSE) +
  labs(
    title = "Vehicle Efficiency",
    subtitle = "Fuel consumption vs weight",
    caption = "Source: mtcars dataset"
  )

generate_plot_story(p, provider = "ollama", model = "deepseek-r1")

story <- generate_plot_story(p, provider = "azure", model = "gpt-4.1-mini", max_tokens = 300)
# To use as subtitle:
p + ggplot2::labs(subtitle = story)
```
  
```{r tests-generate_plot_story}
test_that("generate_plot_story works", {
  expect_true(inherits(generate_plot_story, "function"))
})
```






# Report Rendering

 
## slugify

```{r function-slugify}
#' slugify
#'
#' Convert string to slug
#'
#' @param x the string
#'
#' @return slugified string
#' @importFrom stringr str_to_lower str_replace_all str_remove_all str_squish
#' @importFrom stringi stri_enc_toutf8
#' @export
slugify <- function(x) {
  x |>
    stringi::stri_enc_toutf8() |>
    # Remove extra whitespace first
    stringr::str_squish() |>
    # Convert to lowercase
    stringr::str_to_lower() |>
    # Replace accented characters with their ASCII equivalents
    stringr::str_replace_all("[àáâãäå]", "a") |>
    stringr::str_replace_all("[èéêë]", "e") |>
    stringr::str_replace_all("[ìíîï]", "i") |>
    stringr::str_replace_all("[òóôõöø]", "o") |>
    stringr::str_replace_all("[ùúûü]", "u") |>
    stringr::str_replace_all("[ñ]", "n") |>
    stringr::str_replace_all("[ç]", "c") |>
    stringr::str_replace_all("[ýÿ]", "y") |>
    stringr::str_replace_all("[ž]", "z") |>
    # Remove all non-alphanumeric characters except spaces, underscores, AND hyphens
    stringr::str_replace_all("[^a-z0-9\\s_-]", "") |>
    # Replace spaces and underscores with hyphens
    stringr::str_replace_all("[\\s_]+", "-") |>
    # Remove consecutive hyphens
    stringr::str_replace_all("-+", "-") |>
    # Remove leading/trailing hyphens
    stringr::str_remove_all("^-|-$")
}
```
  
```{r example-slugify}
strings <- c("Café au Lait", "Niño Español", "Data_Science_Project", "--test--string--")
slugify(strings)
```
  
```{r tests-slugify}
test_that("slugify works", {
  expect_true(inherits(slugify, "function"))
})
```
  
  
  
## Generate Section Summary
   
  
```{r function-generate_section_summary}
#' Generate Section Summary from Plot Stories
#'
#' Aggregates individual plot stories into a cohesive section summary.
#'
#' @param stories A character vector of stories generated from plots within the section.
#' @param section_name The title of the section (e.g., "Population Overview").
#' @param provider Optional provider (openai, gemini, anthropic). Auto-detected if NULL.
#' @param model Optional model name.
#' @param max_tokens Max tokens for the summary.
#'
#' @return A character string containing the section summary.
#' @export
#' @examples
#' \dontrun{
#' stories <- c("Story 1...", "Story 2...")
#' summary <- generate_section_summary(stories, "Population")
#' }
generate_section_summary <- function(stories,
                                     section_name,
                                     provider = NULL,
                                     model = NULL,
                                     max_tokens = 400) {
  if (length(valid_stories) == 0) {
    return(paste0("No AI insights available for ", section_name, "."))
  }
  
  # Handle list objects (if passed from new generate_plot_story)
  # Iterate and extract $long_desc
  story_texts <- sapply(valid_stories, function(s) {
    if (is.list(s) && !is.null(s$long_desc)) {
      return(s$long_desc)
    } else if (is.character(s)) {
      return(s)
    } else {
      return("")
    }
  })
  story_texts <- story_texts[story_texts != ""]

  combined_text <- paste(story_texts, collapse = "\n\n")

  system_prompt <- paste0(
    "You are the Lead Editor for the UNHCR Global Report. ",
    "Your task is to synthesize multiple data insights into a cohesive narrative section that reads exactly like a chapter from the 'Global Trends' or 'Mid-Year Trends' report.\n\n",
    "### WRITING INSTRUCTIONS:\n",
    "- **Synthesis:** Do not just list the insights. Weave them together into a story of displacement, protection, or solutions.\n",
    "- **Context:** Connect specific data points to broader global themes (e.g., the impact of the Sudan crisis, the war in Ukraine, climate shocks).\n",
    "- **Language:** Use phrases like 'Behind these stark numbers...', 'The data reveals...', 'This constitutes a rise of...'.\n",
    "- **Focus:** Prioritize the magnitude of displacement, the burden on host communities, and the gap between needs and funding.\n"
  )

  prompt <- paste0(
    "Section Topic: ", section_name, "\n\n",
    "Input Data Narratives:\n", combined_text, "\n\n",
    "TASK: Write a cohesive section summary (approx ", max_tokens, " tokens) for this topic. ",
    "Ensure the tone is formal, humanitarian, and data-driven. Avoid bullet points; use prose."
  )

  # Auto-detect provider if not specified
  if (is.null(provider)) {
    if (!is.na(Sys.getenv("OPENAI_API_KEY", unset = NA_character_))) {
      provider <- "openai"
    } else if (!is.na(Sys.getenv("GEMINI_API_KEY", unset = NA_character_))) {
      provider <- "gemini"
    } else if (!is.na(Sys.getenv("ANTHROPIC_API_KEY", unset = NA_character_))) {
      provider <- "anthropic"
    } else {
      return(paste0("AI summary for ", section_name, " skipped (no API key)."))
    }
  }

  provider <- tolower(provider)

  if (is.null(model)) {
    model <- switch(provider,
      openai = "gpt-4o-mini",
      gemini = "gemini-2.0-flash",
      anthropic = "claude-3-5-sonnet-latest",
      ollama = "phi3:latest",
      return("AI summary skipped (invalid provider).")
    )
  }

  chat <- tryCatch(
    {
      switch(provider,
        openai = ellmer::chat_openai(model = model, system_prompt = system_prompt),
        gemini = ellmer::chat_google_gemini(system_prompt = system_prompt, model = model),
        anthropic = ellmer::chat_anthropic(model = model, system_prompt = system_prompt),
        ollama = ellmer::chat_ollama(model = model, system_prompt = system_prompt),
        stop("Invalid provider")
      )
    },
    error = function(e) {
      NULL
    }
  )

  if (is.null(chat)) {
    return("AI summary failed (chat init).")
  }

  response <- tryCatch(
    chat$chat(prompt),
    error = function(e) {
      "AI summary failed (API error)."
    }
  )

  return(response)
}
```
  
```{r example-generate_section_summary}
# generate_section_summary()
```
  
```{r tests-generate_section_summary}
test_that("generate_section_summary works", {
  expect_true(inherits(generate_section_summary, "function"))
})
```
 
## Generate Report Summary
    
  
```{r function-generate_report_summary}
#' Generate Report Executive Summary
#'
#' Aggregates section summaries into a final executive summary.
#'
#' @param section_summaries A named list or character vector of section summaries.
#' @param country_name Name of the country.
#' @param year Year of the report.
#' @param provider Optional provider.
#' @param model Optional model.
#' @param max_tokens Max tokens.
#'
#' @return A character string containing the executive summary.
#' @export
#' @examples
#' \dontrun{
#' sections <- list("Population" = "Summary...", "Asylum" = "Summary...")
#' report_summary <- generate_report_summary(sections, "Colombia", 2022)
#' }
generate_report_summary <- function(section_summaries,
                                    country_name,
                                    year,
                                    provider = NULL,
                                    model = NULL,
                                    max_tokens = 400) {
  valid_summaries <- section_summaries[!is.na(section_summaries) &
    !grepl("AI summary.*skipped", section_summaries)]

  if (length(valid_summaries) == 0) {
    return("No AI executive summary available.")
  }

  combined_text <- paste(names(valid_summaries), ":\n", valid_summaries, collapse = "\n\n")

  system_prompt <- paste0(
    "You are the UNHCR High Commissioner's speechwriter and lead strategist. ",
    "You are summarizing the entire data report for an executive audience (donors, press, member states).\n\n",
    "### OBJECTIVE:\n",
    "Produce a 'Key Trends' executive summary that highlights the most critical statistics and their humanitarian implications.\n\n",
    "### STYLE GUIDE:\n",
    "- **Urgency:** Convey the scale of the emergency (e.g., 'A world in turmoil', 'Record highs').\n",
    "- **Responsibility:** Highlight the contribution of host countries (e.g., 'Low- and middle-income countries continue to host the majority...').\n",
    "- **Solutions:** Mention resettlement, returns, or pathways if data is present, but be realistic about the gaps.\n",
    "- **Format:** Start with a powerful opening statement. Follow with 3-4 distinct paragraphs covering the main themes (Displacement, Solutions, Funding/Gaps)."
  )

  prompt <- paste0(
    "Country/Context: ", country_name, "\n",
    "Year: ", year, "\n\n",
    "Section Summaries:\n", combined_text, "\n\n",
    "TASK: Draft the Executive Summary (approx ", max_tokens, " tokens) for this report. ",
    "Synthesize the provided section summaries into a high-level overview. ",
    "Highlight the total figures and the primary drivers of change."
  )

  # Auto-detect provider if not specified
  if (is.null(provider)) {
    if (!is.na(Sys.getenv("OPENAI_API_KEY", unset = NA_character_))) {
      provider <- "openai"
    } else if (!is.na(Sys.getenv("GEMINI_API_KEY", unset = NA_character_))) {
      provider <- "gemini"
    } else if (!is.na(Sys.getenv("ANTHROPIC_API_KEY", unset = NA_character_))) {
      provider <- "anthropic"
    } else {
      return("Executive summary skipped (no API key).")
    }
  }

  provider <- tolower(provider)

  if (is.null(model)) {
    model <- switch(provider,
      openai = "gpt-4o-mini",
      gemini = "gemini-2.0-flash",
      anthropic = "claude-3-5-sonnet-latest",
      ollama = "phi3:latest",
      return("AI summary skipped (invalid provider).")
    )
  }

  chat <- tryCatch(
    {
      switch(provider,
        openai = ellmer::chat_openai(model = model, system_prompt = system_prompt),
        gemini = ellmer::chat_google_gemini(system_prompt = system_prompt, model = model),
        anthropic = ellmer::chat_anthropic(model = model, system_prompt = system_prompt),
        ollama = ellmer::chat_ollama(model = model, system_prompt = system_prompt),
        stop("Invalid provider")
      )
    },
    error = function(e) {
      NULL
    }
  )

  if (is.null(chat)) {
    return("AI summary failed (chat init).")
  }

  response <- tryCatch(
    chat$chat(prompt),
    error = function(e) {
      "AI summary failed (API error)."
    }
  )

  return(response)
}
```
  
```{r example-generate_report_summary}
# generate_report_summary()
```
  
```{r tests-generate_report_summary}
test_that("generate_report_summary works", {
  expect_true(inherits(generate_report_summary, "function"))
})
```
      
          

---



# Refresh package
<!--
# There can be development actions

Create a chunk with 'development' actions

- The chunk needs to be named `development` or `dev`
- It contains functions that are used for package development only
- Note that you may want to store most of these functions in the 0-dev_history.Rmd file

These are only included in the present flat template file, their content will not be part of the package anywhere else.
-->

```{r development-inflate, eval=FALSE}
# Keep eval=FALSE to avoid infinite loop in case you hit the knit button
# Execute in the console directly

# remotes::install_github("thinkr-open/checkhelper")
# checkhelper::print_globals()
# tools::showNonASCIIfile("dev/flat_ai_reporting.Rmd")
fusen::inflate(flat_file = "dev/flat_ai_reporting.Rmd", vignette_name = "Analysis with AI", overwrite = "yes")
pkgdown::build_site()
```
